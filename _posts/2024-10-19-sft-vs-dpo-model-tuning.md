---
layout: single
title: "SFT vs DPO: 실전에서 배운 모델 튜닝 전략"
date: 2024-10-19
categories: [AI, Engineering]
tags: [SFT, DPO, Fine-tuning, LLM, 모델튜닝]
---

교육 AI 서비스를 개발하면서 영어 문법 태깅 모델의 정확도를 개선해야 하는 과제를 맡았다.
특히 "it-that 강조구문" 같은 특정 문법 패턴에서 오류가 10문장당 1번 정도 발생했고,
이를 0%에 가깝게 줄여야 했다.

1차로 SFT(Supervised Fine-Tuning)로 튜닝한 모델이었기에, 추가 개선을 위해 두 가지 방법을 고민했다
- SFT 재튜닝 (데이터 증량)
- DPO(Direct Preference Optimization) 추가 튜닝

결론부터 말하자면, **DPO는 효과가 없었고 결국 SFT로 돌아왔다**.
이 과정에서 얻은 교훈을 공유한다.

## 왜 DPO를 선택했는가?

당시 DPO를 선택한 이유는 합리적이었다

1. **적은 데이터로도 효과적**: 10개, 100개, 300개 수준의 작은 데이터셋으로도 성능 개선 가능
2. **비용 효율성**: SFT 대비 적은 비용으로 시도 가능
3. **Catastrophic Forgetting 우려 감소**: 기존 학습 내용 손실 위험이 낮음
4. **빠른 검증**: 신속하게 효과를 확인하고 다음 단계 결정 가능

특히 이미 1차 SFT로 전반적인 성능은 확보된 상태였기에,
"세부 조정"만 필요하다고 판단했다.

## DPO 실험 결과

10개 → 100개 → 300개로 데이터를 점진적으로 늘려가며 튜닝을 진행했다.

**결과: 오류율 개선 없음**

문법 오류가 개선되지 않았다. 왜일까?

## SFT vs DPO의 본질적 차이

실패 후 근본적인 차이를 다시 정리했다

### SFT (Supervised Fine-Tuning)
- **학습 방식**: "정답이 이거야"를 직접 가르침
- **적합한 태스크**: 정답이 명확한 경우 (문법, 번역, 형식 등)
- **효과**: 특정 패턴에 대한 확실한 학습

### DPO (Direct Preference Optimization)
- **학습 방식**: "A보다 B가 더 나아"라는 선호도를 학습
- **적합한 태스크**: 정답이 모호하거나 여러 답이 가능한 경우 (스타일, 톤, 창의성 등)
- **효과**: 전반적인 품질 향상, 미묘한 선호도 반영

## 핵심 교훈: 태스크 특성에 맞는 방법론 선택

우리의 문제는 **"it-that 강조구문을 올바르게 태깅하는가?"**라는 명확한 정답이 있는 태스크였다.

- **틀린 태깅**: [가주어 It] ❌
- **올바른 태깅**: [IT-THAT, 강조구문] ✅

이런 이분법적 정답 문제에는 DPO의 "선호도 학습"이 아니라
SFT의 "정답 학습"이 필요했다.

## 최종 해결 방안: SFT 재튜닝

결국 다음과 같이 SFT 재튜닝을 진행하기로 했다

### 학습 데이터 구성 (총 3,000개)
- **문제 구문 예시**: 300개 (it-that 강조구문 등)
- **기존 학습 데이터**: 2,700개

### 왜 기존 데이터를 포함했는가?

새로운 데이터만 학습하면 **Catastrophic Forgetting** 현상이 발생한다.
기존에 잘 하던 것을 잊어버리는 것이다. 따라서:
- 새로운 패턴 학습 (300개)
- 기존 성능 유지 (2,700개)

두 가지를 동시에 달성하기 위해 혼합 구성했다.

### 예상 비용 및 리스크
- **비용**: 약 $450 (약 60만원)
- **리스크**: 3,000개로도 목표 달성 보장 없음
- **대안 전략**: 소규모(300개) 테스트 후 효과 검증 → 확장

## 300개 SFT 테스트 결과

본격적인 3,000개 학습 후에 300개 테스트 셋으로 테스트를 진행했다.

- **테스트 범위**: 50개 문장
- **결과**: 3개 오류 발생
- **오류율**: 약 6%
- **목표 대비**: 미달성 (목표 0%)

완벽하지는 않았지만 초기 10%에서 6%로 개선되었다.
이는 SFT 방식이 올바른 방향임을 확인시켜 주었다.

## 향후 전략: 두 가지 옵션

현재 두 가지 방향을 고민 중이다.

### Option 1: 추가 SFT 튜닝 (데이터 증량)
- **방법**: 학습 데이터를 더 늘려 재시도
- **장점**: 근본적 성능 개선 가능
- **단점**: 추가 비용, 개선 보장 불확실

### Option 2: 프롬프트 개선 (보완적 접근)
- **방법**: Tag Ranker LLM의 프롬프트에 오류 빈발 구문에 대한 추가 검증 로직 삽입
- **장점**: 비용 효율적, 빠른 적용
- **단점**: 근본적 해결보다는 임시방편

## 실전 가이드: SFT vs DPO 선택 기준

내 경험을 바탕으로 한 실용적인 선택 가이드

### SFT를 선택해야 할 때
- ✅ 정답이 명확한 태스크 (문법, 형식, 분류)
- ✅ 특정 패턴을 확실하게 학습시켜야 할 때
- ✅ 정확도가 최우선일 때
- ✅ 충분한 정답 데이터가 있을 때

### DPO를 선택해야 할 때
- ✅ 정답이 모호하거나 여러 답이 가능할 때
- ✅ 스타일, 톤, 창의성 개선이 목표일 때
- ✅ 미묘한 품질 향상이 필요할 때
- ✅ 선호도 데이터(좋은 예시 vs 나쁜 예시)를 구축하기 쉬울 때
- ✅ 기존 모델 성능 유지가 중요할 때

### 주의사항
- **목표 명확화**: "무엇을 개선할 것인가?"를 먼저 정의
- **태스크 특성 분석**: 정답형 vs 선호형
- **비용-효과 검토**: 빠른 실험으로 검증 후 확장
- **하이브리드 접근**: SFT(기본 성능) + DPO(품질 조정)도 가능

## 마치며

처음엔 "비용이 적게 들고 빠른" DPO가 매력적이었다.
하지만 **방법론의 효율성보다 태스크와의 적합성이 더 중요**하다는 것을 배웠다.

좋은 도구를 잘못된 문제에 적용하면 아무리 노력해도 효과가 없다.

1. 먼저 해결하려는 문제의 본질을 파악하라
2. 그에 맞는 방법론을 선택하라
3. 소규모 실험으로 빠르게 검증하라
4. 효과가 없다면 방법을 바꾸는 것을 두려워하지 마라

---

*이 글은 IHFB AI 연구팀에서의 실무 경험을 기록한 개인 후기입니다.*
